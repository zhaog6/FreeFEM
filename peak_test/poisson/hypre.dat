Sender: LSF System <lsfadmin@c3u15n02>
Subject: Job 35711: <mywork> in cluster <hpc.lsec.cc.ac.cn> Done

Job <mywork> was submitted from host <ln01> by user <zhaogang> in cluster <hpc.lsec.cc.ac.cn>.
Job was executed on host(s) <36*c3u15n02>, in queue <batch>, as user <zhaogang> in cluster <hpc.lsec.cc.ac.cn>.
                            <36*c3u26n02>
                            <36*c4u05n02>
                            <36*c6u15n03>
                            <36*c4u05n04>
                            <36*c3u03n03>
                            <36*c1u10n02>
                            <36*c3u19n02>
                            <36*c2u17n03>
                            <36*c1u15n04>
                            <36*c1u26n02>
                            <36*c1u26n03>
                            <36*c2u05n03>
                            <36*a6u08n02>
                            <36*a6u08n04>
                            <36*c1u08n01>
                            <36*c1u08n02>
                            <36*c2u22n03>
                            <36*c3u01n02>
                            <36*c6u24n04>
                            <36*c3u12n02>
                            <36*c3u12n03>
                            <36*c3u12n04>
                            <36*c2u10n02>
                            <36*c2u10n04>
                            <36*c3u17n01>
                            <36*c3u17n03>
                            <36*c3u17n04>
                            <36*c2u15n02>
                            <36*c2u15n04>
                            <36*c6u01n01>
                            <36*c6u01n04>
</share/home/zhaogang> was used as the home directory.
</share/home/zhaogang/zg/FreeFEM/Labs/peak_test/poisson> was used as the working directory.
Started at Fri Oct  2 18:17:50 2020
Results reported on Fri Oct  2 18:21:25 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec.hydra -genvall /share/home/zhaogang/zg/FreeFEM/New_Version/FreeFem-sources/src/mpi/FreeFem++-mpi poisson-3d-PETSc.edp -v 0 -nn 64 -refi 3 -log_view
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   220067.86 sec.
    Max Memory :                                 2062076 MB
    Average Memory :                             1180476.50 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2 MB
    Max Processes :                              1184
    Max Threads :                                2336

The output (if any) follows:

Petsc Release Version 3.13.5, Sep 01, 2020 
       The PETSc Team
    petsc-maint@mcs.anl.gov
 https://www.mcs.anl.gov/petsc/
See docs/changes/index.html for recent updates.
See docs/faq.html for problems.
See docs/manualpages/index.html for help. 
Libraries linked from /share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib
----------------------------------------
Building distributed mesh: 69.7199 s
Creating dA: 2.82888 s
Assembling dA: 7.84729 s
Assembling b: 0.733075 s
  0 KSP Residual norm 4.114595295404e+01 
  1 KSP Residual norm 7.710723223096e+00 
  2 KSP Residual norm 1.827299177101e+00 
  3 KSP Residual norm 9.493790266101e-01 
  4 KSP Residual norm 4.474808384990e-01 
  5 KSP Residual norm 2.171248996551e-01 
  6 KSP Residual norm 1.051786210687e-01 
  7 KSP Residual norm 5.193165295067e-02 
  8 KSP Residual norm 2.581617223347e-02 
  9 KSP Residual norm 1.269474154287e-02 
 10 KSP Residual norm 6.225643446001e-03 
 11 KSP Residual norm 3.038718124189e-03 
 12 KSP Residual norm 1.476711747541e-03 
 13 KSP Residual norm 7.187967481258e-04 
 14 KSP Residual norm 3.507952384644e-04 
 15 KSP Residual norm 1.717421608590e-04 
 16 KSP Residual norm 8.388988615740e-05 
 17 KSP Residual norm 4.094160131751e-05 
 18 KSP Residual norm 2.011274155699e-05 
 19 KSP Residual norm 9.939586659596e-06 
 20 KSP Residual norm 4.900460862438e-06 
 21 KSP Residual norm 2.447402858666e-06 
 22 KSP Residual norm 1.229542520285e-06 
 23 KSP Residual norm 6.180963850657e-07 
 24 KSP Residual norm 3.087851967863e-07 
Linear solve converged due to CONVERGED_RTOL iterations 24
 --- system solved with PETSc (in 38.0782)
Computing nDofs: 0.0108919 s
----------------------------------
Th.nt = 805306368
Dofs: 1.35006e+08
----------------------------------
ttttttttttttttttttttttttttttttttttttttttttttt
Elapsed time is 190.047 s
ttttttttttttttttttttttttttttttttttttttttttttt
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/share/home/zhaogang/zg/FreeFEM/New_Version/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named c3u15n02 with 1152 processors, by zhaogang Fri Oct  2 18:21:06 2020
Using Petsc Release Version 3.13.5, Sep 01, 2020 

                         Max       Max/Min     Avg       Total
Time (sec):           1.902e+02     1.000   1.902e+02
Objects:              5.300e+01     1.000   5.300e+01
Flop:                 2.617e+08     1.245   2.366e+08  2.725e+11
Flop/sec:             1.376e+06     1.246   1.244e+06  1.433e+09
MPI Messages:         5.415e+02     6.119   3.515e+02  4.049e+05
MPI Message Lengths:  5.370e+06     2.880   1.224e+04  4.957e+09
MPI Reductions:       7.900e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.9021e+02 100.0%  2.7254e+11 100.0%  4.049e+05 100.0%  1.224e+04      100.0%  7.200e+01  91.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          1 1.0 2.4049e-02255.4 0.00e+00 0.0 7.1e+03 4.0e+00 0.0e+00  0  0  2  0  0   0  0  2  0  0     0
BuildTwoSidedF         2 1.0 1.7505e-0110.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               24 1.0 2.1252e-01 2.2 9.00e+07 1.2 3.4e+05 1.1e+04 0.0e+00  0 34 84 72  0   0 34 84 72  0 440709
MatConvert             1 1.0 3.5715e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  5   0  0  0  0  6     0
MatAssemblyBegin       3 1.0 1.7639e-01 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         3 1.0 3.1687e+00 9.0 0.00e+00 0.0 2.1e+04 3.5e+03 8.0e+00  1  0  5  2 10   1  0  5  2 11     0
MatGetRowIJ            2 1.0 1.8287e-04191.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               24 1.0 1.1256e+00 1.4 7.77e+07 1.2 0.0e+00 0.0e+00 2.4e+01  1 30  0  0 30   1 30  0  0 33 71965
VecNorm               25 1.0 8.8106e-01 1.3 6.48e+06 1.2 0.0e+00 0.0e+00 2.5e+01  0  2  0  0 32   0  2  0  0 35  7662
VecScale              25 1.0 3.9131e-0244.0 3.24e+06 1.2 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 86253
VecCopy                1 1.0 4.4181e-0321.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                28 1.0 5.2472e-0218.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                1 1.0 8.4801e-0353.3 2.59e+05 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 31841
VecMAXPY              25 1.0 1.6054e-01 2.8 8.40e+07 1.2 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0 544938
VecScatterBegin       24 1.0 3.8362e-0233.1 0.00e+00 0.0 3.4e+05 1.1e+04 0.0e+00  0  0 84 72  0   0  0 84 72  0     0
VecScatterEnd         24 1.0 9.6823e-02128.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          25 1.0 8.9423e-01 1.3 9.72e+06 1.2 0.0e+00 0.0e+00 2.5e+01  0  4  0  0 32   0  4  0  0 35 11323
SFSetGraph             1 1.0 2.3127e-05 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 4.7688e-02 3.1 0.00e+00 0.0 2.1e+04 3.5e+03 0.0e+00  0  0  5  2  0   0  0  5  2  0     0
SFBcastOpBegin        24 1.0 3.8296e-0233.9 0.00e+00 0.0 3.4e+05 1.1e+04 0.0e+00  0  0 84 72  0   0  0 84 72  0     0
SFBcastOpEnd          24 1.0 9.6771e-02134.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack                24 1.0 2.1361e-0242.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              24 1.0 3.4332e-05 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.0447e-0226.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.8086e+01 1.0 2.62e+08 1.2 3.4e+05 1.1e+04 5.3e+01 20100 84 72 67  20100 84 72 74  7156
KSPGMRESOrthog        24 1.0 1.1882e+00 1.4 1.55e+08 1.2 0.0e+00 0.0e+00 2.4e+01  1 59  0  0 30   1 59  0  0 33 136347
PCSetUp                1 1.0 3.2662e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00 17  0  0  0  5  17  0  0  0  6     0
PCApply               25 1.0 3.6037e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5     41220232     0.
         Vec Scatter     1              1          800     0.
              Vector    40             40     33218568     0.
           Index Set     2              2        54668     0.
   Star Forest Graph     1              1         1120     0.
       Krylov Solver     1              1        18640     0.
      Preconditioner     1              1         1440     0.
              Viewer     2              1          840     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.33991e-05
Average time for zero size MPI_Send(): 4.1806e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_monitor
-ksp_rtol 1e-8
-log_view
-nn 64
-pc_hypre_boomeramg_agg_nl 2
-pc_hypre_boomeramg_coarsen_type hmis
-pc_hypre_boomeramg_interp_type ext+i
-pc_hypre_boomeramg_no_CF
-pc_hypre_boomeramg_P_max 4
-pc_type hypre
-refi 3
-v 0
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/share/soft/mvapich2-2.3b_gcc/bin/mpicc --with-cxx=/share/soft/mvapich2-2.3b_gcc/bin/mpic++ --with-fc=/share/soft/mvapich2-2.3b_gcc/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/share/soft/intel_2018_update1/mkl/include --with-blaslapack-lib="-Wl,-rpath,/share/soft/intel_2018_update1/mkl/lib/intel64 -L/share/soft/intel_2018_update1/mkl/lib/intel64 -lmkl_rt -lmkl_sequential -lmkl_core   -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-ml --download-parmetis --download-superlu_dist --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-hpddm-commit=ce6ce80 --download-scalapack --download-mumps PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2020-09-11 01:40:36 on ln01 
Machine characteristics: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
Using PETSc directory: /share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/soft/mvapich2-2.3b_gcc/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -mtune=native  
Using Fortran compiler: /share/soft/mvapich2-2.3b_gcc/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -mtune=native    
-----------------------------------------

Using include paths: -I/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/include -I/share/soft/intel_2018_update1/mkl/include
-----------------------------------------

Using C linker: /share/soft/mvapich2-2.3b_gcc/bin/mpicc
Using Fortran linker: /share/soft/mvapich2-2.3b_gcc/bin/mpif90
Using libraries: -Wl,-rpath,/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -L/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -lpetsc -Wl,-rpath,/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -L/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -Wl,-rpath,/share/soft/intel_2018_update1/mkl/lib/intel64 -L/share/soft/intel_2018_update1/mkl/lib/intel64 -Wl,-rpath,/soft/mvapich2-2.3b-gcc/lib -L/soft/mvapich2-2.3b-gcc/lib -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.7 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.7 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/daal/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/daal/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.4 -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu_dist -lml -lmkl_rt -lmkl_sequential -lmkl_core -liomp5 -lpthread -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lm -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------



PS:

Read file <err.dat> for stderr output of this job.

