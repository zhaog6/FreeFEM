Sender: LSF System <lsfadmin@c3u15n02>
Subject: Job 35715: <mywork> in cluster <hpc.lsec.cc.ac.cn> Done

Job <mywork> was submitted from host <ln01> by user <zhaogang> in cluster <hpc.lsec.cc.ac.cn>.
Job was executed on host(s) <36*c3u15n02>, in queue <batch>, as user <zhaogang> in cluster <hpc.lsec.cc.ac.cn>.
                            <36*c3u26n02>
                            <36*c6u15n03>
                            <36*c4u05n04>
                            <36*c2u01n03>
                            <36*c2u01n04>
                            <36*c3u08n02>
                            <36*c1u10n03>
                            <36*c1u15n02>
                            <36*c6u03n03>
                            <36*c6u03n04>
                            <36*c6u08n01>
                            <36*c6u08n02>
                            <36*c1u26n01>
                            <36*c4u10n02>
                            <36*c6u19n03>
                            <36*c2u05n03>
                            <36*a6u08n01>
                            <36*c1u03n02>
                            <36*c1u03n03>
                            <36*c1u03n04>
                            <36*a6u08n04>
                            <36*c1u08n02>
                            <36*c1u08n04>
                            <36*c3u24n01>
                            <36*a6u19n02>
                            <36*c2u22n03>
                            <36*c1u19n01>
                            <36*c1u19n03>
                            <36*c4u03n01>
                            <36*c4u03n02>
                            <36*c3u01n04>
</share/home/zhaogang> was used as the home directory.
</share/home/zhaogang/zg/FreeFEM/Labs/peak_test/poisson> was used as the working directory.
Started at Fri Oct  2 18:57:08 2020
Results reported on Fri Oct  2 19:00:50 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec.hydra -genvall /share/home/zhaogang/zg/FreeFEM/New_Version/FreeFem-sources/src/mpi/FreeFem++-mpi poisson-3d-PETSc.edp -v 0 -nn 64 -refi 3 -log_view
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   226518.20 sec.
    Max Memory :                                 2058659 MB
    Average Memory :                             1194631.38 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Processes :                              1184
    Max Threads :                                2336

The output (if any) follows:

Petsc Release Version 3.13.5, Sep 01, 2020 
       The PETSc Team
    petsc-maint@mcs.anl.gov
 https://www.mcs.anl.gov/petsc/
See docs/changes/index.html for recent updates.
See docs/faq.html for problems.
See docs/manualpages/index.html for help. 
Libraries linked from /share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib
----------------------------------------
Building distributed mesh: 71.132 s
Creating dA: 3.63707 s
Assembling dA: 7.76136 s
Assembling b: 0.732898 s
  0 KSP Residual norm 4.165138595458e+01 
  1 KSP Residual norm 8.070845326933e+00 
  2 KSP Residual norm 3.350884679135e+00 
  3 KSP Residual norm 1.725087272687e+00 
  4 KSP Residual norm 8.738228865729e-01 
  5 KSP Residual norm 4.167117836651e-01 
  6 KSP Residual norm 1.985604140828e-01 
  7 KSP Residual norm 9.525056174762e-02 
  8 KSP Residual norm 4.650799630223e-02 
  9 KSP Residual norm 2.248844512996e-02 
 10 KSP Residual norm 1.089787622798e-02 
 11 KSP Residual norm 5.285990972156e-03 
 12 KSP Residual norm 2.569766552851e-03 
 13 KSP Residual norm 1.245322906414e-03 
 14 KSP Residual norm 6.025953883582e-04 
 15 KSP Residual norm 2.950507752706e-04 
 16 KSP Residual norm 1.465926743670e-04 
 17 KSP Residual norm 7.405501854751e-05 
 18 KSP Residual norm 3.794459638961e-05 
 19 KSP Residual norm 1.941318643269e-05 
 20 KSP Residual norm 9.904768736639e-06 
 21 KSP Residual norm 5.028549677021e-06 
 22 KSP Residual norm 2.540284726272e-06 
 23 KSP Residual norm 1.275250452700e-06 
 24 KSP Residual norm 6.346293798569e-07 
 25 KSP Residual norm 3.127465497221e-07 
Linear solve converged due to CONVERGED_RTOL iterations 25
 --- system solved with PETSc (in 37.2516)
Computing nDofs: 0.0127831 s
----------------------------------
Th.nt = 805306368
Dofs: 1.35006e+08
----------------------------------
ttttttttttttttttttttttttttttttttttttttttttttt
Elapsed time is 195.361 s
ttttttttttttttttttttttttttttttttttttttttttttt
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/share/home/zhaogang/zg/FreeFEM/New_Version/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named c3u15n02 with 1152 processors, by zhaogang Fri Oct  2 19:00:29 2020
Using Petsc Release Version 3.13.5, Sep 01, 2020 

                         Max       Max/Min     Avg       Total
Time (sec):           1.960e+02     1.001   1.959e+02
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.326e+08     1.245   1.198e+08  1.381e+11
Flop/sec:             6.767e+05     1.244   6.119e+05  7.049e+08
MPI Messages:         5.605e+02     6.126   3.638e+02  4.191e+05
MPI Message Lengths:  5.532e+06     2.879   1.218e+04  5.107e+09
MPI Reductions:       1.060e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.9587e+02 100.0%  1.3807e+11 100.0%  4.191e+05 100.0%  1.218e+04      100.0%  9.900e+01  93.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          1 1.0 9.7080e-02 6.3 0.00e+00 0.0 7.1e+03 4.0e+00 0.0e+00  0  0  2  0  0   0  0  2  0  0     0
BuildTwoSidedF         2 1.0 2.3974e-01 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               25 1.0 2.1281e-01 2.1 9.37e+07 1.2 3.5e+05 1.1e+04 0.0e+00  0 71 85 73  0   0 71 85 73  0 458460
MatConvert             1 1.0 6.8624e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  4   0  0  0  0  4     0
MatAssemblyBegin       3 1.0 2.4183e-01 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         3 1.0 2.4796e+00 4.4 0.00e+00 0.0 2.1e+04 3.5e+03 8.0e+00  1  0  5  1  8   1  0  5  1  8     0
MatGetRowIJ            2 1.0 3.3128e-033473.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecTDot               50 1.0 5.1413e-01 1.6 1.30e+07 1.2 0.0e+00 0.0e+00 5.0e+01  0 10  0  0 47   0 10  0  0 51 26259
VecNorm               26 1.0 5.9847e-01 1.4 6.74e+06 1.2 0.0e+00 0.0e+00 2.6e+01  0  5  0  0 25   0  5  0  0 26 11730
VecCopy                2 1.0 5.2798e-0311.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                28 1.0 7.1825e-0232.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               50 1.0 2.5365e-02 1.6 1.30e+07 1.2 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 10  0  0  0 532250
VecAYPX               24 1.0 3.1385e-02 6.5 6.22e+06 1.2 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0 206479
VecScatterBegin       25 1.0 3.5419e-0213.8 0.00e+00 0.0 3.5e+05 1.1e+04 0.0e+00  0  0 85 73  0   0  0 85 73  0     0
VecScatterEnd         25 1.0 8.0643e-02134.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 2.5988e-05 5.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 1.0065e-01 4.1 0.00e+00 0.0 2.1e+04 3.5e+03 0.0e+00  0  0  5  1  0   0  0  5  1  0     0
SFBcastOpBegin        25 1.0 3.5378e-0214.1 0.00e+00 0.0 3.5e+05 1.1e+04 0.0e+00  0  0 85 73  0   0  0 85 73  0     0
SFBcastOpEnd          25 1.0 8.0598e-02141.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack                25 1.0 1.4637e-0217.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              25 1.0 9.0837e-0595.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 3.1329e-02138.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.7260e+01 1.0 1.33e+08 1.2 3.5e+05 1.1e+04 8.0e+01 19100 85 73 75  19100 85 73 81  3705
PCSetUp                1 1.0 3.4290e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00 17  0  0  0  4  17  0  0  0  4     0
PCApply               26 1.0 1.9214e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5     41220232     0.
         Vec Scatter     1              1          800     0.
              Vector     9              9      4710472     0.
           Index Set     2              2        54668     0.
   Star Forest Graph     1              1         1120     0.
       Krylov Solver     1              1         1472     0.
      Preconditioner     1              1         1440     0.
              Viewer     2              1          840     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.88351e-05
Average time for zero size MPI_Send(): 4.19012e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_monitor
-ksp_rtol 1e-8
-ksp_type cg
-log_view
-nn 64
-pc_hypre_boomeramg_agg_nl 2
-pc_hypre_boomeramg_coarsen_type hmis
-pc_hypre_boomeramg_interp_type ext+i
-pc_hypre_boomeramg_no_CF
-pc_hypre_boomeramg_P_max 4
-pc_type hypre
-refi 3
-v 0
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/share/soft/mvapich2-2.3b_gcc/bin/mpicc --with-cxx=/share/soft/mvapich2-2.3b_gcc/bin/mpic++ --with-fc=/share/soft/mvapich2-2.3b_gcc/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/share/soft/intel_2018_update1/mkl/include --with-blaslapack-lib="-Wl,-rpath,/share/soft/intel_2018_update1/mkl/lib/intel64 -L/share/soft/intel_2018_update1/mkl/lib/intel64 -lmkl_rt -lmkl_sequential -lmkl_core   -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-ml --download-parmetis --download-superlu_dist --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-hpddm-commit=ce6ce80 --download-scalapack --download-mumps PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2020-09-11 01:40:36 on ln01 
Machine characteristics: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
Using PETSc directory: /share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/soft/mvapich2-2.3b_gcc/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -mtune=native  
Using Fortran compiler: /share/soft/mvapich2-2.3b_gcc/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -mtune=native    
-----------------------------------------

Using include paths: -I/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/include -I/share/soft/intel_2018_update1/mkl/include
-----------------------------------------

Using C linker: /share/soft/mvapich2-2.3b_gcc/bin/mpicc
Using Fortran linker: /share/soft/mvapich2-2.3b_gcc/bin/mpif90
Using libraries: -Wl,-rpath,/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -L/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -lpetsc -Wl,-rpath,/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -L/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -Wl,-rpath,/share/soft/intel_2018_update1/mkl/lib/intel64 -L/share/soft/intel_2018_update1/mkl/lib/intel64 -Wl,-rpath,/soft/mvapich2-2.3b-gcc/lib -L/soft/mvapich2-2.3b-gcc/lib -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.7 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.7 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/daal/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/daal/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.4 -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu_dist -lml -lmkl_rt -lmkl_sequential -lmkl_core -liomp5 -lpthread -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lm -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------



PS:

Read file <err.dat> for stderr output of this job.

