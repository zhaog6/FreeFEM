Sender: LSF System <lsfadmin@c3u10n04>
Subject: Job 284520: <mywork> in cluster <hpc.lsec.cc.ac.cn> Done

Job <mywork> was submitted from host <ln02> by user <zhaogang> in cluster <hpc.lsec.cc.ac.cn>.
Job was executed on host(s) <36*c3u10n04>, in queue <batch>, as user <zhaogang> in cluster <hpc.lsec.cc.ac.cn>.
                            <36*c1u05n01>
                            <36*c6u15n03>
                            <36*a6u26n02>
                            <36*a6u26n04>
                            <36*c4u10n02>
                            <36*c6u19n03>
                            <36*c2u05n04>
                            <36*c1u03n01>
                            <36*a6u08n02>
                            <36*c1u03n04>
                            <36*a6u19n01>
                            <36*a6u19n02>
                            <36*c3u24n03>
                            <36*c2u10n02>
                            <36*c6u01n01>
                            <36*c2u26n02>
                            <36*c6u12n03>
                            <36*c6u12n04>
                            <36*c6u17n01>
                            <36*c6u17n02>
                            <36*c6u17n03>
                            <36*c3u05n01>
                            <36*c2u03n02>
                            <36*c1u01n03>
                            <36*c1u01n04>
                            <36*a6u12n01>
                            <36*a6u12n02>
                            <36*c3u22n01>
                            <36*c1u12n03>
                            <36*c3u22n03>
                            <36*c3u22n04>
</share/home/zhaogang> was used as the home directory.
</share/home/zhaogang/zg/FreeFEM/Labs/peak_test/elasticity> was used as the working directory.
Started at Tue Oct 20 18:15:36 2020
Results reported on Tue Oct 20 18:19:39 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec.hydra -genvall /share/home/zhaogang/zg/FreeFEM/New_Version/FreeFem-sources/src/mpi/FreeFem++-mpi elasticity-3d-PETSc.edp -v 0 -nn 10 -refi 3 -ksp_view -ksp_monitor_true_residual -log_view
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   267776.00 sec.
    Max Memory :                                 973138 MB
    Average Memory :                             479666.66 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Processes :                              1184
    Max Threads :                                2336

The output (if any) follows:

Petsc Release Version 3.13.5, Sep 01, 2020 
       The PETSc Team
    petsc-maint@mcs.anl.gov
 https://www.mcs.anl.gov/petsc/
See docs/changes/index.html for recent updates.
See docs/faq.html for problems.
See docs/manualpages/index.html for help. 
Libraries linked from /share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib
----------------------------------------
  0 KSP preconditioned resid norm 2.544926696353e+02 true resid norm 1.990756740361e+01 ||r(i)||/||b|| 1.000000000000e+00
  1 KSP preconditioned resid norm 7.394766450950e+00 true resid norm 1.723923319495e+05 ||r(i)||/||b|| 8.659638239789e+03
  2 KSP preconditioned resid norm 4.750524227513e+00 true resid norm 1.150699763321e+05 ||r(i)||/||b|| 5.780212820538e+03
  3 KSP preconditioned resid norm 3.695598396418e+00 true resid norm 5.054986779485e+04 ||r(i)||/||b|| 2.539228765122e+03
  4 KSP preconditioned resid norm 2.623121522066e+00 true resid norm 3.626597287359e+04 ||r(i)||/||b|| 1.821717949679e+03
  5 KSP preconditioned resid norm 1.984109905242e+00 true resid norm 2.259915376165e+04 ||r(i)||/||b|| 1.135204181580e+03
  6 KSP preconditioned resid norm 1.478106006253e+00 true resid norm 1.913548481060e+04 ||r(i)||/||b|| 9.612166279609e+02
  7 KSP preconditioned resid norm 1.101702538590e+00 true resid norm 1.301822733721e+04 ||r(i)||/||b|| 6.539336059137e+02
  8 KSP preconditioned resid norm 8.474857859157e-01 true resid norm 8.762991081520e+03 ||r(i)||/||b|| 4.401839212124e+02
  9 KSP preconditioned resid norm 6.987967752066e-01 true resid norm 6.161845602076e+03 ||r(i)||/||b|| 3.095227798128e+02
 10 KSP preconditioned resid norm 5.776131343344e-01 true resid norm 4.056779050259e+03 ||r(i)||/||b|| 2.037807517117e+02
 11 KSP preconditioned resid norm 4.534097646632e-01 true resid norm 3.193875380679e+03 ||r(i)||/||b|| 1.604352413294e+02
 12 KSP preconditioned resid norm 3.318689983514e-01 true resid norm 2.451179538600e+03 ||r(i)||/||b|| 1.231280291009e+02
 13 KSP preconditioned resid norm 2.197815080491e-01 true resid norm 1.695901882702e+03 ||r(i)||/||b|| 8.518880525777e+01
 14 KSP preconditioned resid norm 1.482702720307e-01 true resid norm 1.294985395999e+03 ||r(i)||/||b|| 6.504990638706e+01
 15 KSP preconditioned resid norm 1.023496359703e-01 true resid norm 9.152063986178e+02 ||r(i)||/||b|| 4.597278914409e+01
 16 KSP preconditioned resid norm 6.535778610192e-02 true resid norm 6.421319690743e+02 ||r(i)||/||b|| 3.225567223034e+01
 17 KSP preconditioned resid norm 4.157584841272e-02 true resid norm 4.823305902829e+02 ||r(i)||/||b|| 2.422850469392e+01
 18 KSP preconditioned resid norm 2.513669373029e-02 true resid norm 3.489866830941e+02 ||r(i)||/||b|| 1.753035295668e+01
 19 KSP preconditioned resid norm 1.312664659700e-02 true resid norm 2.605731290825e+02 ||r(i)||/||b|| 1.308914965850e+01
 20 KSP preconditioned resid norm 6.344277454476e-03 true resid norm 1.921210494843e+02 ||r(i)||/||b|| 9.650654225561e+00
 21 KSP preconditioned resid norm 5.336853138125e-03 true resid norm 1.344770501101e+02 ||r(i)||/||b|| 6.755071947451e+00
 22 KSP preconditioned resid norm 6.167161962680e-03 true resid norm 1.012139904098e+02 ||r(i)||/||b|| 5.084196796009e+00
 23 KSP preconditioned resid norm 6.075246163030e-03 true resid norm 7.236046192339e+01 ||r(i)||/||b|| 3.634821897439e+00
 24 KSP preconditioned resid norm 5.769973462330e-03 true resid norm 4.995947629821e+01 ||r(i)||/||b|| 2.509572128293e+00
 25 KSP preconditioned resid norm 5.037414170621e-03 true resid norm 3.669954083655e+01 ||r(i)||/||b|| 1.843497002547e+00
 26 KSP preconditioned resid norm 3.962230896750e-03 true resid norm 2.578364785569e+01 ||r(i)||/||b|| 1.295168180670e+00
 27 KSP preconditioned resid norm 2.821698165848e-03 true resid norm 1.774794611846e+01 ||r(i)||/||b|| 8.915175701096e-01
 28 KSP preconditioned resid norm 1.849972516765e-03 true resid norm 1.258010354924e+01 ||r(i)||/||b|| 6.319257041402e-01
 29 KSP preconditioned resid norm 1.146091987965e-03 true resid norm 8.680812347801e+00 ||r(i)||/||b|| 4.360559063700e-01
 30 KSP preconditioned resid norm 6.535009616689e-04 true resid norm 6.081066683776e+00 ||r(i)||/||b|| 3.054650807147e-01
 31 KSP preconditioned resid norm 3.111579636022e-04 true resid norm 4.247134989500e+00 ||r(i)||/||b|| 2.133427406470e-01
 32 KSP preconditioned resid norm 1.514744369348e-04 true resid norm 2.917027304004e+00 ||r(i)||/||b|| 1.465285659902e-01
 33 KSP preconditioned resid norm 1.583090718684e-04 true resid norm 2.111911827408e+00 ||r(i)||/||b|| 1.060858810417e-01
 34 KSP preconditioned resid norm 1.824255198527e-04 true resid norm 1.481026175914e+00 ||r(i)||/||b|| 7.439513557272e-02
 35 KSP preconditioned resid norm 1.787296799616e-04 true resid norm 1.029489802422e+00 ||r(i)||/||b|| 5.171349073196e-02
 36 KSP preconditioned resid norm 1.562455651929e-04 true resid norm 7.632524691076e-01 ||r(i)||/||b|| 3.833981589178e-02
 37 KSP preconditioned resid norm 1.268332628609e-04 true resid norm 5.561086123350e-01 ||r(i)||/||b|| 2.793453369065e-02
 38 KSP preconditioned resid norm 9.600238951343e-05 true resid norm 4.065299539550e-01 ||r(i)||/||b|| 2.042087542456e-02
 39 KSP preconditioned resid norm 6.735700148835e-05 true resid norm 3.056347354942e-01 ||r(i)||/||b|| 1.535269123031e-02
 40 KSP preconditioned resid norm 4.290914167889e-05 true resid norm 2.234756089706e-01 ||r(i)||/||b|| 1.122566129954e-02
 41 KSP preconditioned resid norm 2.522208095890e-05 true resid norm 1.631424999483e-01 ||r(i)||/||b|| 8.194999250322e-03
 42 KSP preconditioned resid norm 1.401913627898e-05 true resid norm 1.169453173002e-01 ||r(i)||/||b|| 5.874415237646e-03
 43 KSP preconditioned resid norm 7.209002387002e-06 true resid norm 8.178914605462e-02 ||r(i)||/||b|| 4.108445014723e-03
 44 KSP preconditioned resid norm 3.369086780975e-06 true resid norm 5.871273447662e-02 ||r(i)||/||b|| 2.949267144813e-03
 45 KSP preconditioned resid norm 1.465125922812e-06 true resid norm 4.076102878075e-02 ||r(i)||/||b|| 2.047514292146e-03
Linear solve converged due to CONVERGED_RTOL iterations 45
KSP Object: 1152 MPI processes
  type: cg
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-08, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1152 MPI processes
  type: gamg
    type is MULTIPLICATIVE, levels=6 cycles=v
      Cycles per PCApply=1
      Using externally compute Galerkin coarse grid matrices
      GAMG specific options
        Threshold for dropping small values in graph on each level =   0.02   0.02   0.02   0.02  
        Threshold scaling factor for each level not specified = 1.
        AGG specific options
          Symmetric graph false
          Number of levels to square graph 4
          Number smoothing steps 1
        Complexity:    grid = 1.28211
  Coarse grid solver -- level -------------------------------
    KSP Object: (mg_coarse_) 1152 MPI processes
      type: preonly
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_coarse_) 1152 MPI processes
      type: bjacobi
        number of blocks = 1152
        Local solve is same for all blocks, in the following KSP and PC objects:
      KSP Object: (mg_coarse_sub_) 1 MPI processes
        type: preonly
        maximum iterations=1, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (mg_coarse_sub_) 1 MPI processes
        type: lu
          out-of-place factorization
          tolerance for zero pivot 2.22045e-14
          using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
          matrix ordering: nd
          factor fill ratio given 5., needed 1.
            Factored matrix follows:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=6
                package used to perform factorization: petsc
                total: nonzeros=1296, allocated nonzeros=1296
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 8 nodes, limit used is 5
        linear system matrix = precond matrix:
        Mat Object: 1 MPI processes
          type: seqaij
          rows=36, cols=36, bs=6
          total: nonzeros=1296, allocated nonzeros=1296
          total number of mallocs used during MatSetValues calls=0
            using I-node routines: found 8 nodes, limit used is 5
      linear system matrix = precond matrix:
      Mat Object: 1152 MPI processes
        type: mpiaij
        rows=36, cols=36, bs=6
        total: nonzeros=1296, allocated nonzeros=1296
        total number of mallocs used during MatSetValues calls=0
          using I-node (on process 0) routines: found 8 nodes, limit used is 5
  Down solver (pre-smoother) on level 1 -------------------------------
    KSP Object: (mg_levels_1_) 1152 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.163747, max = 1.80122
        eigenvalues estimate via cg min 0.0043637, max 1.63747
        eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
        KSP Object: (mg_levels_1_esteig_) 1152 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=2, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_1_) 1152 MPI processes
      type: sor
        type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
      linear system matrix = precond matrix:
      Mat Object: 1152 MPI processes
        type: mpiaij
        rows=192, cols=192, bs=6
        total: nonzeros=23112, allocated nonzeros=23112
        total number of mallocs used during MatSetValues calls=0
          using I-node (on process 0) routines: found 8 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  Down solver (pre-smoother) on level 2 -------------------------------
    KSP Object: (mg_levels_2_) 1152 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.162537, max = 1.78791
        eigenvalues estimate via cg min 0.00835725, max 1.62537
        eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
        KSP Object: (mg_levels_2_esteig_) 1152 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=2, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_2_) 1152 MPI processes
      type: sor
        type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
      linear system matrix = precond matrix:
      Mat Object: 1152 MPI processes
        type: mpiaij
        rows=4428, cols=4428, bs=6
        total: nonzeros=3415752, allocated nonzeros=3415752
        total number of mallocs used during MatSetValues calls=0
          using I-node (on process 0) routines: found 5 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  Down solver (pre-smoother) on level 3 -------------------------------
    KSP Object: (mg_levels_3_) 1152 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.157089, max = 1.72798
        eigenvalues estimate via cg min 0.0261322, max 1.57089
        eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
        KSP Object: (mg_levels_3_esteig_) 1152 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=2, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_3_) 1152 MPI processes
      type: sor
        type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
      linear system matrix = precond matrix:
      Mat Object: 1152 MPI processes
        type: mpiaij
        rows=150156, cols=150156, bs=6
        total: nonzeros=171050040, allocated nonzeros=171050040
        total number of mallocs used during MatSetValues calls=0
          using nonscalable MatPtAP() implementation
          using I-node (on process 0) routines: found 9 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  Down solver (pre-smoother) on level 4 -------------------------------
    KSP Object: (mg_levels_4_) 1152 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.15706, max = 1.72766
        eigenvalues estimate via cg min 0.0322192, max 1.5706
        eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
        KSP Object: (mg_levels_4_esteig_) 1152 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=2, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_4_) 1152 MPI processes
      type: sor
        type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
      linear system matrix = precond matrix:
      Mat Object: 1152 MPI processes
        type: mpiaij
        rows=5049378, cols=5049378, bs=6
        total: nonzeros=2831160420, allocated nonzeros=2831160420
        total number of mallocs used during MatSetValues calls=0
          using nonscalable MatPtAP() implementation
          using I-node (on process 0) routines: found 1192 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  Down solver (pre-smoother) on level 5 -------------------------------
    KSP Object: (mg_levels_5_) 1152 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.233232, max = 2.56555
        eigenvalues estimate via cg min 0.0419037, max 2.33232
        eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
        KSP Object: (mg_levels_5_esteig_) 1152 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=2, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_5_) 1152 MPI processes
      type: sor
        type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
      linear system matrix = precond matrix:
      Mat Object: 1152 MPI processes
        type: mpiaij
        rows=124498563, cols=124498563, bs=3
        total: nonzeros=10654110729, allocated nonzeros=10654110729
        total number of mallocs used during MatSetValues calls=0
          has attached near null space
          using I-node (on process 0) routines: found 32764 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  linear system matrix = precond matrix:
  Mat Object: 1152 MPI processes
    type: mpiaij
    rows=124498563, cols=124498563, bs=3
    total: nonzeros=10654110729, allocated nonzeros=10654110729
    total number of mallocs used during MatSetValues calls=0
      has attached near null space
      using I-node (on process 0) routines: found 32764 nodes, limit used is 5
 --- system solved with PETSc (in 73.6794)
------------------------------------
# of Elements: 30720000
# of Dofs: 124498563
lambda = 3.10345e+08
mu = 3.44828e+07
------------------------------------
ttttttttttttttttttttttttttttttttttttttttttttt
Elapsed time is 124.566 s
ttttttttttttttttttttttttttttttttttttttttttttt
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/share/home/zhaogang/zg/FreeFEM/New_Version/FreeFem-sources/src/mpi/FreeFem++-mpi on a  named c3u10n04 with 1152 processors, by zhaogang Tue Oct 20 18:19:31 2020
Using Petsc Release Version 3.13.5, Sep 01, 2020 

                         Max       Max/Min     Avg       Total
Time (sec):           2.300e+02     1.001   2.299e+02
Objects:              8.980e+02     1.002   8.960e+02
Flop:                 1.616e+10     1.454   1.358e+10  1.564e+13
Flop/sec:             7.027e+07     1.453   5.905e+07  6.802e+10
MPI Messages:         4.648e+04     6.915   2.384e+04  2.747e+07
MPI Message Lengths:  2.981e+08     4.016   7.735e+03  2.125e+11
MPI Reductions:       1.392e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.2991e+02 100.0%  1.5639e+13 100.0%  2.747e+07 100.0%  7.735e+03      100.0%  1.385e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         44 1.0 8.5271e+00 1.0 0.00e+00 0.0 2.5e+05 4.0e+00 0.0e+00  4  0  1  0  0   4  0  1  0  0     0
BuildTwoSidedF       226 1.0 5.0114e+00 2.6 0.00e+00 0.0 1.6e+05 2.1e+05 0.0e+00  1  0  1 16  0   1  0  1 16  0     0
MatMult             1111 1.0 1.9112e+01 1.8 7.77e+09 1.4 1.8e+07 6.1e+03 0.0e+00  6 48 64 51  0   6 48 64 51  0 391084
MatMultAdd           230 1.0 6.2349e+00 7.9 3.84e+08 1.3 1.9e+06 1.1e+03 0.0e+00  2  3  7  1  0   2  3  7  1  0 62751
MatMultTranspose     230 1.0 4.9092e+00 3.4 3.85e+08 1.3 1.9e+06 1.1e+03 0.0e+00  1  3  7  1  0   1  3  7  1  0 79751
MatSolve              46 0.0 2.4700e-04 0.0 1.18e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   476
MatSOR               975 1.0 9.1620e+00 1.6 4.69e+09 1.5 0.0e+00 0.0e+00 0.0e+00  3 29  0  0  0   3 29  0  0  0 500871
MatLUFactorSym         1 1.0 1.1629e-021283.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 1.0306e-025403.2 3.05e+04 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     3
MatScale              15 1.0 4.5775e-02 2.0 1.09e+07 1.3 8.0e+04 1.4e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0 241178
MatResidual          230 1.0 4.8056e+00 2.0 1.32e+09 1.5 3.7e+06 5.0e+03 0.0e+00  1  8 13  9  0   1  8 13  9  0 261506
MatAssemblyBegin     116 1.0 5.0306e+00 2.4 0.00e+00 0.0 1.6e+05 2.1e+05 0.0e+00  1  0  1 16  0   1  0  1 16  0     0
MatAssemblyEnd       116 1.0 2.0049e+01 1.9 3.08e+06 5.3 6.5e+05 7.2e+02 1.4e+02  7  0  2  0 10   7  0  2  0 10    88
MatGetRowIJ            1 0.0 5.5361e-03 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMat        6 1.0 2.0039e+00 1.0 0.00e+00 0.0 2.1e+04 2.0e+03 7.8e+01  1  0  0  0  6   1  0  0  0  6     0
MatGetOrdering         1 0.0 6.4330e-03 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCoarsen             5 1.0 4.1177e-01 1.2 0.00e+00 0.0 1.5e+06 7.5e+02 5.9e+01  0  0  6  1  4   0  0  6  1  4     0
MatZeroEntries         7 1.0 1.1178e-0122.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                9 1.3 5.9859e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 7.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAXPY                5 1.0 1.4490e-01 1.5 7.68e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5367
MatTranspose          10 1.0 1.0557e-01 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultSym         15 1.0 3.2250e+00 1.2 0.00e+00 0.0 3.3e+05 1.2e+04 4.0e+01  1  0  1  2  3   1  0  1  2  3     0
MatMatMultNum          5 1.0 4.6828e-01 1.1 1.72e+08 1.5 8.0e+04 3.0e+04 0.0e+00  0  1  0  1  0   0  1  0  1  0 349902
MatPtAPSymbolic        5 1.0 5.7296e+00 1.0 0.00e+00 0.0 4.2e+05 9.0e+04 3.5e+01  2  0  2 18  3   2  0  2 18  3     0
MatPtAPNumeric         5 1.0 1.0167e+01 1.0 2.47e+09 1.6 2.7e+05 1.2e+05 2.0e+01  4 15  1 15  1   4 15  1 15  1 227619
MatTrnMatMultSym       4 1.0 2.0407e+00 1.0 0.00e+00 0.0 1.9e+05 2.1e+04 3.6e+01  1  0  1  2  3   1  0  1  2  3     0
MatTrnMatMultNum       4 1.0 1.1492e+00 1.1 6.10e+07 1.4 1.1e+05 7.1e+04 8.0e+00  0  0  0  4  1   0  0  0  4  1 51164
MatGetLocalMat        21 1.0 9.5061e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetBrAoCol         15 1.0 9.7706e-0119.8 0.00e+00 0.0 5.6e+05 6.4e+04 0.0e+00  0  0  2 17  0   0  0  2 17  0     0
VecMDot                5 1.0 2.4443e-01 2.7 3.67e+06 1.3 0.0e+00 0.0e+00 5.0e+00  0  0  0  0  0   0  0  0  0  0 15280
VecTDot              300 1.0 6.4345e+00 1.2 3.28e+07 1.3 0.0e+00 0.0e+00 3.0e+02  3  0  0  0 22   3  0  0  0 22  5176
VecNorm              154 1.0 1.1469e+01 1.5 2.71e+07 1.3 0.0e+00 0.0e+00 1.5e+02  4  0  0  0 11   4  0  0  0 11  2398
VecScale               6 1.0 6.9320e-0334.3 7.35e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 107760
VecCopy              758 1.0 1.7781e-01 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               890 1.0 6.5618e-0213.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              290 1.0 8.3935e-02 2.3 3.23e+07 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 390610
VecAYPX             1560 1.0 2.3963e-01 2.7 6.81e+07 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 288292
VecAXPBYCZ           460 1.0 1.2017e-01 4.6 5.89e+07 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 496478
VecMAXPY               3 1.0 5.4346e-0237.8 2.94e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 54980
VecAssemblyBegin     186 1.0 1.1511e-01 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd       186 1.0 1.8787e-04 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult      55 1.0 1.6125e-02 3.6 1.41e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 88481
VecScatterBegin     1766 1.0 3.0511e-01 7.3 0.00e+00 0.0 2.4e+07 4.9e+03 0.0e+00  0  0 88 56  0   0  0 88 56  0     0
VecScatterEnd       1766 1.0 1.3237e+01 2.1 5.22e+05 5.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0    20
VecSetRandom           5 1.0 8.0944e-0231.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           6 1.0 4.2332e-01 2.3 2.20e+06 1.3 0.0e+00 0.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0  5294
SFSetGraph            44 1.0 2.1720e-04 5.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               44 1.0 8.6636e+00 1.0 0.00e+00 0.0 7.5e+05 7.9e+02 0.0e+00  4  0  3  0  0   4  0  3  0  0     0
SFBcastOpBegin      1605 1.0 3.0112e-01 7.3 0.00e+00 0.0 2.4e+07 5.0e+03 0.0e+00  0  0 86 55  0   0  0 86 55  0     0
SFBcastOpEnd        1605 1.0 1.1143e+01 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
SFReduceBegin        230 1.0 1.2210e-02 8.2 0.00e+00 0.0 1.9e+06 1.1e+03 0.0e+00  0  0  7  1  0   0  0  7  1  0     0
SFReduceEnd          230 1.0 3.8832e+00 9.8 5.22e+05 5.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    67
SFPack              1835 1.0 2.0021e-01 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            1835 1.0 3.2265e-03 4.6 5.22e+05 5.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 81211
KSPSetUp              18 1.0 5.7597e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1     0
KSPSolve               1 1.0 7.3613e+01 1.0 1.61e+10 1.5 2.7e+07 7.6e+03 1.3e+03 32100100 98 96  32100100 98 97 212319
PCGAMGGraph_AGG        5 1.0 2.6778e+00 1.0 1.56e+07 1.5 2.7e+05 8.2e+02 8.0e+01  1  0  1  0  6   1  0  1  0  6  5574
PCGAMGCoarse_AGG       5 1.0 3.6752e+00 1.0 6.10e+07 1.4 2.1e+06 6.5e+03 1.2e+02  2  0  8  7  9   2  0  8  7  9 15999
PCGAMGProl_AGG         5 1.0 8.0270e-01 1.0 0.00e+00 0.0 2.3e+06 2.7e+03 3.7e+02  0  0  9  3 27   0  0  9  3 27     0
PCGAMGPOpt_AGG         5 1.0 4.7696e+00 1.0 4.81e+08 1.4 1.2e+06 8.5e+03 1.9e+02  2  3  4  5 13   2  3  4  5 14 96325
GAMG: createProl       5 1.0 1.1872e+01 1.0 5.57e+08 1.4 6.0e+06 5.2e+03 7.6e+02  5  3 22 15 55   5  3 22 15 55 44909
  Graph               10 1.0 2.6496e+00 1.0 1.56e+07 1.5 2.7e+05 8.2e+02 8.0e+01  1  0  1  0  6   1  0  1  0  6  5634
  MIS/Agg              5 1.0 4.1181e-01 1.2 0.00e+00 0.0 1.5e+06 7.5e+02 5.9e+01  0  0  6  1  4   0  0  6  1  4     0
  SA: col data         5 1.0 2.8674e-01 1.0 0.00e+00 0.0 2.3e+06 2.2e+03 3.3e+02  0  0  8  2 24   0  0  8  2 24     0
  SA: frmProl0         5 1.0 4.8745e-01 1.1 0.00e+00 0.0 8.3e+04 1.7e+04 2.0e+01  0  0  0  1  1   0  0  0  1  1     0
  SA: smooth           5 1.0 3.0117e+00 1.0 1.81e+08 1.4 4.1e+05 1.5e+04 5.2e+01  1  1  1  3  4   1  1  1  3  4 57487
GAMG: partLevel        5 1.0 1.8725e+01 1.0 2.47e+09 1.6 7.1e+05 9.8e+04 2.1e+02  8 15  3 33 15   8 15  3 33 15 123586
  repartition          3 1.0 2.9171e+00 1.0 0.00e+00 0.0 2.4e+04 1.7e+03 1.5e+02  1  0  0  0 11   1  0  0  0 11     0
  Invert-Sort          3 1.0 2.7838e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.8e+01  0  0  0  0  1   0  0  0  0  1     0
  Move A               3 1.0 8.7717e-01 1.0 0.00e+00 0.0 6.3e+03 6.3e+03 4.2e+01  0  0  0  0  3   0  0  0  0  3     0
  Move P               3 1.0 1.1638e+00 1.0 0.00e+00 0.0 1.4e+04 5.7e+01 4.5e+01  1  0  0  0  3   1  0  0  0  3     0
PCSetUp                2 1.0 3.0758e+01 1.0 3.03e+09 1.6 6.7e+06 1.5e+04 1.0e+03 13 18 24 47 72  13 18 24 47 72 92574
PCSetUpOnBlocks       46 1.0 1.6672e-0222.0 3.05e+04 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     2
PCApply               46 1.0 3.0214e+01 1.1 1.11e+10 1.4 1.9e+07 4.3e+03 1.6e+02 12 69 71 39 11  12 69 71 39 12 356456
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix   177            177    332988224     0.
      Matrix Coarsen     5              5         3180     0.
   Matrix Null Space     1              1          696     0.
         Vec Scatter    39             39        31200     0.
              Vector   470            470    102898928     0.
           Index Set   118            118       519620     0.
   Star Forest Graph    44             44        49280     0.
       Krylov Solver    18             18        30072     0.
      Preconditioner    13             13        12764     0.
              Viewer     3              2         1680     0.
         PetscRandom    10             10         6460     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.85966e-05
Average time for zero size MPI_Send(): 4.67876e-06
#PETSc Option Table entries:
-ksp_converged_reason
-ksp_monitor_true_residual
-ksp_rtol 1e-8
-ksp_type cg
-ksp_view
-log_view
-mg_levels_esteig_ksp_type cg
-nn 10
-pc_gamg_esteig_ksp_type cg
-pc_gamg_square_graph 4
-pc_gamg_threshold 0.02
-pc_type gamg
-refi 3
-v 0
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r MAKEFLAGS= --with-debugging=0 COPTFLAGS="-O3 -mtune=native" CXXOPTFLAGS="-O3 -mtune=native" FOPTFLAGS="-O3 -mtune=native" --with-cxx-dialect=C++11 --with-ssl=0 --with-x=0 --with-fortran-bindings=0 --with-cudac=0 --with-cc=/share/soft/mvapich2-2.3b_gcc/bin/mpicc --with-cxx=/share/soft/mvapich2-2.3b_gcc/bin/mpic++ --with-fc=/share/soft/mvapich2-2.3b_gcc/bin/mpif90 --with-scalar-type=real --with-blaslapack-include=/share/soft/intel_2018_update1/mkl/include --with-blaslapack-lib="-Wl,-rpath,/share/soft/intel_2018_update1/mkl/lib/intel64 -L/share/soft/intel_2018_update1/mkl/lib/intel64 -lmkl_rt -lmkl_sequential -lmkl_core   -liomp5 -lpthread" --download-metis --download-ptscotch --download-hypre --download-ml --download-parmetis --download-superlu_dist --download-suitesparse --download-tetgen --download-slepc --download-hpddm --download-hpddm-commit=ce6ce80 --download-scalapack --download-mumps PETSC_ARCH=fr
-----------------------------------------
Libraries compiled on 2020-09-11 01:40:36 on ln01 
Machine characteristics: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
Using PETSc directory: /share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/soft/mvapich2-2.3b_gcc/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -mtune=native  
Using Fortran compiler: /share/soft/mvapich2-2.3b_gcc/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -mtune=native    
-----------------------------------------

Using include paths: -I/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/include -I/share/soft/intel_2018_update1/mkl/include
-----------------------------------------

Using C linker: /share/soft/mvapich2-2.3b_gcc/bin/mpicc
Using Fortran linker: /share/soft/mvapich2-2.3b_gcc/bin/mpif90
Using libraries: -Wl,-rpath,/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -L/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -lpetsc -Wl,-rpath,/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -L/share/home/zhaogang/zg/FreeFEM/New_Version/ff-petsc/r/lib -Wl,-rpath,/share/soft/intel_2018_update1/mkl/lib/intel64 -L/share/soft/intel_2018_update1/mkl/lib/intel64 -Wl,-rpath,/soft/mvapich2-2.3b-gcc/lib -L/soft/mvapich2-2.3b-gcc/lib -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.7 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.7 -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/daal/lib/intel64_lin -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/daal/lib/intel64_lin -Wl,-rpath,/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.4 -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu_dist -lml -lmkl_rt -lmkl_sequential -lmkl_core -liomp5 -lpthread -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lparmetis -lmetis -ltet -lm -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lrt -lquadmath -lstdc++ -ldl
-----------------------------------------



PS:

Read file <err.dat> for stderr output of this job.

